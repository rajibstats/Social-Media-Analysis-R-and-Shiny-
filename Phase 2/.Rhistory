create_train_test & lt;- function(data, size = 0.8, train = TRUE) {
n_row = nrow(data)
total_row = size * n_row
train_sample & lt; - 1: total_row
if (train == TRUE) {
return (data[train_sample, ])
} else {
return (data[-train_sample, ])
}
}
create_train_test & lt;- function(data, size = 0.8, train = TRUE) {
n_row = nrow(data)
total_row = size * n_row
train_sample & lt; - 1: total_row
if (train == TRUE) {
return (data[train_sample, ])
} else {
return (data[-train_sample, ])
}
}
create_train_test = function(data, size = 0.8, train = TRUE) {
n_row = nrow(data)
total_row = size * n_row
train_sample = 1: total_row
if (train == TRUE) {
return (data[train_sample, ])
} else {
return (data[-train_sample, ])
}
}
data_train <- create_train_test(clean_titanic, 0.8, train = TRUE)
data_test <- create_train_test(clean_titanic, 0.8, train = FALSE)
dim(data_train)
dim(data_test)
prop.table(table(data_train$survived))
prop.table(table(data_test$survived))
library(rpart)
library(rpart.plot)
install.packages("rpart.plot")
library(rpart.plot)
fit <- rpart(survived~., data = data_train, method = 'class')
rpart.plot(fit, extra = 106)
predict_unseen <-predict(fit, data_test, type = 'class')
predict_unseen
table_mat <- table(data_test$survived, predict_unseen)
table_mat
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
accuracy_Test
accuracy_tune <- function(fit) {
predict_unseen <- predict(fit, data_test, type = 'class')
table_mat <- table(data_test$survived, predict_unseen)
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
accuracy_Test
}
control <- rpart.control(minsplit = 4,
minbucket = round(5 / 3),
maxdepth = 3,
cp = 0)
tune_fit <- rpart(survived~., data = data_train, method = 'class', control = control)
accuracy_tune(tune_fit)
5/14
-(3/5)log2(3/5)-(2/5)log2(2/5)
-(3/5)*log2(3/5)-(2/5)*log2(2/5)
3/sqrt(5)
?randomforest
library(randomForest)
?randomForest
#read data file
mydata= read.csv("E:/RUMAN/study/german_credit.csv")
# Check attributes of data
str(mydata)
# Check number of rows and columns
dim(mydata)
# Make dependent variable as a factor (categorical)
mydata$Creditability = as.factor(mydata$Creditability)
# Split data into training (70%) and validation (30%)
dt = sort(sample(nrow(mydata), nrow(mydata)*.7))
train<-mydata[dt,]
val<-mydata[-dt,] # Check number of rows in training data set
nrow(train)
# To view dataset
edit(train)
# Decision Tree Model
library(rpart)
mtree <- rpart(Creditability~., data = train, method="class", control = rpart.control(minsplit = 20, minbucket = 7, maxdepth = 10, usesurrogate = 2, xval =10 ))
mtree
#Plot tree
plot(mtree)
text(mtree)
#Beautify tree
library(rattle)
library(rpart.plot)
library(RColorBrewer)
#view1
prp(mtree, faclen = 0, cex = 0.8, extra = 1)
#view2 - total count at each node
tot_count <- function(x, labs, digits, varlen)
{paste(labs, "\n\nn =", x$frame$n)}
prp(mtree, faclen = 0, cex = 0.8, node.fun=tot_count)
mtree$cptable
bestcp <- mtree$cptable[which.min(mtree$cptable[,"xerror"]),"CP"]
bestcp
# Prune the tree using the best cp.
pruned <- prune(mtree, cp = bestcp)
pruned
# Plot pruned tree
prp(pruned, faclen = 0, cex = 0.8, extra = 1)
# confusion matrix (training data)
conf.matrix <- table(train$Creditability, predict(pruned,type="class"))
conf.matrix
rownames(conf.matrix) <- paste("Actual", rownames(conf.matrix), sep = ":")
colnames(conf.matrix) <- paste("Pred", colnames(conf.matrix), sep = ":")
print(conf.matrix)
#Scoring
library(ROCR)
val1 = predict(pruned, val, type = "prob")
val1
#Storing Model Performance Scores
pred_val <-prediction(val1[,2],val$Creditability)
pred_val
# Calculating Area under Curve
perf_val <- performance(pred_val,"auc")
perf_val
# Plotting Lift curve
plot(performance(pred_val, measure="lift", x.measure="rpp"), colorize=TRUE)
# Calculating True Positive and False Positive Rate
perf_val <- performance(pred_val, "tpr", "fpr")
perf_val
# Plot the ROC curve
plot(perf_val, col = "green", lwd = 1.5)
#Calculating KS statistics
ks1.tree <- max(attr(perf_val, "y.values")[[1]] - (attr(perf_val, "x.values")[[1]]))
ks1.tree
mydata= read.csv("E:/RUMAN/study/german_credit.csv")
# Check types of variables
str(mydata)
# Check number of rows and columns
dim(mydata)
# Make dependent variable as a factor (categorical)
mydata$Creditability = as.factor(mydata$Creditability)
library(randomForest)
set.seed(71)
rf <-randomForest(Creditability~.,data=mydata, ntree=500)
print(rf)
floor(sqrt(ncol(mydata) - 1))
mtry <- tuneRF(mydata[-1],mydata$Creditability, ntreeTry=500,
stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
mtry[, 2]
min(mtry[, 2])
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
print(mtry)
print(best.m)
set.seed(71)
rf <-randomForest(Creditability~.,data=mydata, mtry=best.m, importance=TRUE,ntree=500)
print(rf)
#Evaluate variable importance
importance(rf)
varImpPlot(rf)
library(ROCR)
perf = prediction(pred1[,2], mydata$Creditability)
pred1=predict(rf,type = "prob")
pred1
library(ROCR)
perf = prediction(pred1[,2], mydata$Creditability)
# 1. Area under curve
auc = performance(perf, "auc")
auc
# 2. True Positive and Negative Rate
pred3 = performance(perf, "tpr","fpr")
# 3. Plot the ROC curve
plot(pred3,main="ROC Curve for Random Forest",col=2,lwd=2)
abline(a=0,b=1,lwd=2,lty=2,col="gray")
require(xgboost)
install.packages("xgboost")
install.packages("Matrix")
require(data.table)
require(xgboost)
require(Matrix)
require(data.table)
if (!require('vcd')) install.packages('vcd')
data(Arthritis)
df <- data.table(Arthritis, keep.rownames = F)
install.packages("data.table")
install.packages("data.table")
require(data.table)
data(Arthritis)
df <- data.table(Arthritis, keep.rownames = F)
?Arthritis
Arthritis
library(data.table)
install.packages("data.table")
library(data.table)
data(Arthritis)
df <- data.table(Arthritis, keep.rownames = F)
library(xgboost)
library(Matrix)
data(Arthritis)
df <- data.table(Arthritis, keep.rownames = F)
data.table(Arthritis, keep.rownames = F)
Arthritis
install.packages("Hmisc")
library(Hmisc)
a = sasxport.get("C:/Users/USER1/Downloads/binary.sas7bdat")
write.csv(a, file="E:/RUMAN/study/binary.csv")
a = sasxport.get("C:\Users\USER1\Downloads\binary.sas7bdat")
#Read Data File
mydata <- read.csv("E:/RUMAN/study/german_credit.csv")
#Summary
summary(mydata)
#Cross Tab
xtabs(~admit + rank, data = mydata)
#Data Preparation
mydata$Creditability <- factor(mydata$Creditability)
# Split data into training (70%) and validation (30%)
dt = sort(sample(nrow(mydata), nrow(mydata)*.7))
train<-mydata[dt,]
val<-mydata[-dt,]
# Check number of rows in training and validation data sets
nrow(train)
nrow(val)
#Run Logistic Regression
mylogistic <- glm(Creditability ~ ., data = train, family = "binomial")
summary(mylogistic)
summary(mylogistic)$coefficient
names(summary(mylogistic))
#Stepwise Logistic Regression
mylogit = step(mylogistic)
#Logistic Regression Coefficient
summary.coeff0 = summary(mylogit)$coefficient
summary.coeff0
#Stepwise Logistic Regression
mylogit = glm(Creditability ~ Account.Balance + Duration.of.Credit..month. +
Payment.Status.of.Previous.Credit + Credit.Amount +
Value.Savings.Stocks + Length.of.current.employment + Instalment.per.cent +
Type.of.apartment + No.of.Credits.at.this.Bank, data = train, family = "binomial")
#Logistic Regression Coefficient
summary.coeff0 = summary(mylogit)$coefficient
summary.coeff0
#Calculating Odd Ratios
OddRatio = exp(coef(mylogit))
OddRatio
summary.coeff = cbind(Variable = row.names(summary.coeff0), OddRatio, summary.coeff0)
summary.coeff
View(summary.coeff)
row.names(summary.coeff) = NULL
#R Function : Standardized Coefficients
stdz.coff <- function (regmodel)
{ b <- summary(regmodel)$coef[-1,1]
sx <- sapply(regmodel$model[-1], sd)
beta <-(3^(1/2))/pi * sx * b
return(beta)
}
std.Coeff = data.frame(Standardized.Coeff = stdz.coff(mylogit))
std.Coeff = cbind(Variable = row.names(std.Coeff), std.Coeff)
row.names(std.Coeff) = NULL
View(std.Coeff)
#Final Summary Report
final = merge(summary.coeff, std.Coeff, by = "Variable", all.x = TRUE)
View(final)
#Prediction
pred = predict(mylogit,val, type = "response")
finaldata = cbind(val, pred)
View(finaldata)
#Storing Model Performance Scores
library(ROCR)
pred_val <-prediction(pred ,finaldata$admit)
pred_val <-prediction(pred ,finaldata$Creditability)
pred_val
# Maximum Accuracy and prob. cutoff against it
acc.perf <- performance(pred_val, "acc")
acc.perf
ind = which.max( slot(acc.perf, "y.values")[[1]])
acc = slot(acc.perf, "y.values")[[1]][ind]
cutoff = slot(acc.perf, "x.values")[[1]][ind]
# Print Results
print(c(accuracy= acc, cutoff = cutoff))
# Calculating Area under Curve
perf_val <- performance(pred_val,"auc")
perf_val
# Plotting Lift curve
plot(performance(pred_val, measure="lift", x.measure="rpp"), colorize=TRUE)
# Plot the ROC curve
perf_val2 <- performance(pred_val, "tpr", "fpr")
plot(perf_val2, col = "green", lwd = 1.5)
#Calculating KS statistics
ks1.tree <- max(attr(perf_val2, "y.values")[[1]] - (attr(perf_val2, "x.values")[[1]]))
ks1.tree
# Print Results
print(c(accuracy= acc, cutoff = cutoff))
# install library
install.packages("neuralnet ")
install.packages("installr")
library(installr)
updateR()
ll = download.file(full_l, "http://40.121.10.212:3300/api/v1/analytics/rawData/client/5a336f194d0808277a808e41/machine/machine-17c77af0-b73c-11e8-99eb-f319cd4ae055/subassembly/componentAnalyzer1/collector/mg1/csv/CSV?startDate=1559812620&endDate=1559816220.csv")
raw.result <- GET(url = "http://40.121.10.212:3300/api/v1/analytics/rawData/client/5a336f194d0808277a808e41/machine/machine-17c77af0-b73c-11e8-99eb-f319cd4ae055/subassembly/componentAnalyzer1/collector/mg1/csv/CSV?startDate=1559812620&endDate=1559816220", path = path)
library(RCurl)
library(XML)
raw.result <- GET(url = "http://40.121.10.212:3300/api/v1/analytics/rawData/client/5a336f194d0808277a808e41/machine/machine-17c77af0-b73c-11e8-99eb-f319cd4ae055/subassembly/componentAnalyzer1/collector/mg1/csv/CSV?startDate=1559812620&endDate=1559816220", path = path)
raw.result <- GET(url = "http://40.121.10.212:3300/api/v1/analytics/rawData/client/5a336f194d0808277a808e41/machine/machine-17c77af0-b73c-11e8-99eb-f319cd4ae055/subassembly/componentAnalyzer1/collector/mg1/csv/CSV?startDate=1559812620&endDate=1559816220")
library(httr)
library(jsonlite)
raw.result <- GET(url = "http://40.121.10.212:3300/api/v1/analytics/rawData/client/5a336f194d0808277a808e41/machine/machine-17c77af0-b73c-11e8-99eb-f319cd4ae055/subassembly/componentAnalyzer1/collector/mg1/csv/CSV?startDate=1559812620&endDate=1559816220")
View(raw.result)
raw.result$cookies
raw.result$headers
raw.result$content
raw.result$handle
raw.result <- read.csv(url = "http://40.121.10.212:3300/api/v1/analytics/rawData/client/5a336f194d0808277a808e41/machine/machine-17c77af0-b73c-11e8-99eb-f319cd4ae055/subassembly/componentAnalyzer1/collector/mg1/csv/CSV?startDate=1559812620&endDate=1559816220.csv")
raw.result <- read.csv(url = "http://40.121.10.212:3300/api/v1/analytics/rawData/client/5a336f194d0808277a808e41/machine/machine-17c77af0-b73c-11e8-99eb-f319cd4ae055/subassembly/componentAnalyzer1/collector/mg1/csv/CSV?startDate=1559812620&endDate=1559816220.csv", header = TRUE, sep = ",")
hh = getURL("http://40.121.10.212:3300/api/v1/analytics/rawData/client/5a336f194d0808277a808e41/machine/machine-17c77af0-b73c-11e8-99eb-f319cd4ae055/subassembly/componentAnalyzer1/collector/mg1/csv/CSV?startDate=1559812620&endDate=1559816220.csv")
gg = strsplit(hh,",")
gg = unlist(strsplit(hh,","))
gg
hh
setwd("E:/RUMAN/new project/QARA_socialmediaanalysis")
################
#data1=read.csv(file.choose())
data1= read.csv("brexit since_2017-03-01 until_2017-03-19.csv")
View(data1)
data_loc=data.frame(data1$Location.Name)
View(data_loc)
names(data_loc)=c("loc")
data_loc[data_loc==""] <- NA
data1_loc <-as.data.frame(na.omit(data_loc))
View(data1_loc)
str(data1_loc)
library(dplyr)
data2_loc=data1_loc %>% distinct(loc)
View(data2_loc)
data3_loc=as.data.frame(data1_loc[!duplicated(data1_loc), ])
names(data3_loc)=c("loc")
# This function geocodes a location (find latitude and longitude) using the Google Maps API
library(ggmap)
library(maps)
register_google(key = "AIzaSyDfYOhXSS2C2LvZxssDndcc6B0fcKSTpek")
#lonlat <- geocode(unique(data1_loc$loc))
lonlat= geocode(as.character(data2_loc$loc))
cities1=cbind(data2_loc, lonlat)
View(cities1)
shiny::runApp()
twitter_query="Afganistan"; twitter_no_of_tweets_to_fetch = 140; twitter_from_date=as.character(Sys.Date() - 7); twitter_to_date=as.character(Sys.Date())
tweets_list = searchTwitter( twitter_query, n = twitter_no_of_tweets_to_fetch, lang = 'en', since = twitter_from_date, until = twitter_to_date )
tweets_df = twListToDF( tweets_list )
#....... Row indexes which has invalid multibyte stings .......
invalid_row_indexes = which( has.invalid.multibyte.string( tweets_df$text, return.elements = T ) )
if( length( invalid_row_indexes ) > 0 ){
final_tweets_df = tweets_df[ -invalid_row_indexes, ]
} else{ final_tweets_df = tweets_df }
positive_words = lexicon$word[ lexicon$polarity == 'positive' ]
negative_words = lexicon$word[ lexicon$polarity == 'negative' ]
negation_words = NULL
sentiment_score_df_0 = score.sentiment( final_tweets_df$text, positive_words, negative_words, negation_words )
sentiment_score_df_1 = inner_join( sentiment_score_df_0, final_tweets_df, by = 'text' )
sentiment_score_df_2 = sentiment_score_df_1[ c( 'created', 'screenName', 'text', 'favorited', 'favoriteCount', 'retweetCount', 'isRetweet', 'retweeted', 'statusSource', 'score' ) ]
sentiment_score_df_3 = sentiment_score_df_2[ !duplicated( sentiment_score_df_2$text ), ]
sentiment_score_df_4 = sentiment_score_df_3 %>%
mutate( source_device = case_when( grepl( 'android', tolower( sentiment_score_df_3$statusSource ) ) ~ 'Android',
grepl( 'iphone', tolower( sentiment_score_df_3$statusSource ) ) ~ 'Iphone',
grepl( 'ipad', tolower( sentiment_score_df_3$statusSource ) ) ~ 'IPad',
grepl( 'ios', tolower( sentiment_score_df_3$statusSource ) ) ~ 'iOS',
grepl( 'web', tolower( sentiment_score_df_3$statusSource ) ) ~ 'Web',
TRUE ~ 'Other' ) ) %>% select( - 'statusSource' )
sentiment_score_df = sentiment_score_df_4 %>%
mutate( polarity = case_when( score > 0  ~ 'Positive',
score < 0  ~ 'Negative',
TRUE ~ 'Neutral' ) )
total_rows = nrow( sentiment_score_df )
positive_score_percent = round( 100*length( which( sentiment_score_df$score > 0 ) )/total_rows, 2 )
neutral_score_percent = round( 100*length( which( sentiment_score_df$score == 0 ) )/total_rows, 2 )
negative_score_percent = round( 100*length( which( sentiment_score_df$score < 0 ) )/total_rows, 2 )
sentiment_distribution = data.frame( Sentiment = c( 'Positive', 'Neutral', 'Negative' ),
Percentage = c( positive_score_percent, neutral_score_percent, negative_score_percent ) )
reach_count = sum( tweets_df$retweetCount ) + nrow( tweets_df )
domain_names_stats = table( paste0( na.omit( unlist( ex_between( tweets_df$statusSource, '<a href=\"', '.com' ) ) ), '.com' ) ) %>%
as.data.frame() %>% `colnames<-` ( c( 'Domains', 'Count' ) ) %>% arrange( desc( Count ) )
top_users = table( tweets_df$screenName ) %>% as.data.frame() %>% `colnames<-` ( c( 'Users', 'Count' ) ) %>% arrange( desc( Count ) ) %>% head( 5 )
source_device_stats = round( 100*prop.table( table( sentiment_score_df$source_device ) ) , 2 ) %>% as.data.frame() %>% `colnames<-` ( c( 'Source_Device', 'Percentage' ) )
share_of_posts_stats = round( 100*prop.table( table( sentiment_score_df$isRetweet ) ) , 2 ) %>% as.data.frame() %>% `colnames<-` ( c( 'Post_Type', 'Percentage' ) )
share_of_posts_stats$Post_Type = ifelse( as.logical( share_of_posts_stats$Post_Type ), 'Retweet', 'Original Post' )
top_posts_df_0 = tweets_df %>% arrange( desc( retweetCount ) ) %>% select( text, retweetCount, screenName ) %>% `colnames<-` ( c( 'Tweets', 'retweetCount', 'screenName' ) )
top_posts_df = top_posts_df_0[ !duplicated( top_posts_df_0[ c( 'Tweets', 'retweetCount' ) ] ), ] %>% head(5)
unique_users = length( unique( tweets_df$screenName ) )
userInfo <- lookupUsers(tweets_df$screenName)  # Batch lookup of user info
userFrame <- twListToDF(userInfo)  # Convert to a nice dF
View(userFrame)
#data_loc=data.frame(data1$Location.Name)
data_loc=data.frame(userFrame$location)
names(data_loc)=c("loc")
data_loc[data_loc==""] <- NA
data1_loc <-as.data.frame(na.omit(data_loc))
library(dplyr)
data2_loc=data1_loc %>% distinct(loc)
data3_loc=as.data.frame(data1_loc[!duplicated(data1_loc), ])
names(data3_loc)=c("loc")
# This function geocodes a location (find latitude and longitude) using the Google Maps API
library(ggmap)
library(maps)
register_google(key = "AIzaSyDfYOhXSS2C2LvZxssDndcc6B0fcKSTpek")
#lonlat <- geocode(unique(data1_loc$loc))
lonlat= geocode(as.character(data2_loc$loc))
cities1=cbind(data2_loc, lonlat)
View(cities1)
names(cities1)
points <- ggmap(cities1) + geom_point(aes(x = lon, y = lat), data = cities1, alpha = .5)
# This function geocodes a location (find latitude and longitude) using the Google Maps API
library(ggmap)
library(maps)
points <- ggmap(cities1) + geom_point(aes(x = lon, y = lat), data = cities1, alpha = .5)
str(cities1)
points <- ggmap(map) + geom_point(aes(x = lon, y = lat), data = cities1, alpha = .5)
points
names(cities1)
leaflet(cities1) %>%
addTiles() %>%
addMarkers(lat = ~lat, lng = ~lon)
library(leaflet)
leaflet(cities1) %>%
addTiles() %>%
addMarkers(lat = ~lat, lng = ~lon)
lurl <- "http://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv?accessType=DOWNLOAD"
data <- read.csv(url, header = TRUE) # takes a minute...
data <- read.csv(lurl, header = TRUE) # takes a minute...
data <- read.csv("Food_Inspections.csv", header = TRUE) # takes a minute...
names(data) <- tolower(names(data))
data1 <- subset(data, risk %in% c("Risk 1 (High)","Risk 2 (Medium)","Risk 3 (Low)"))
View(data1)
data1$risk <- droplevels(data1$risk)
data1 <- data1[1:50,]
library(leaflet)
leaflet(data1) %>%
addTiles() %>%
addMarkers(lat = ~latitude, lng = ~longitude)
View(data1)
leaflet(cities1) %>%
addTiles() %>%
addMarkers(lat = ~lat, lng = ~lon)
cities1 = cities1[complete.cases(cities1), ]
leaflet(cities1) %>%
addTiles() %>%
addMarkers(lat = ~lat, lng = ~lon)
cities1$lon = as.numeric(cities1$lon); cities1$lat = as.numeric(cities1$lat)
leaflet(cities1) %>%
addTiles() %>%
addMarkers(lat = ~lat, lng = ~lon)
leaflet(cities1[c(12,13,24,27,28),]) %>%
addTiles() %>%
addMarkers(lat = ~lat, lng = ~lon)
leaflet(cities1[c(12,13,24,27,28),]) %>%
addTiles() %>%
addMarkers(lat = ~lat, lng = ~lon)
library( maps )
cities1
'location_lat_lon_df'
setwd("E:/RUMAN/new project/QARA_socialmediaanalysis")
runApp()
runApp()
runApp()
runApp()
runApp()
twitter_query="Afganistan";twitter_no_of_tweets_to_fetch=140;twitter_from_date=as.character(Sys.Date() - 7);twitter_to_date=as.character(Sys.Date())
hh = Sentiment_Output_Twitter( twitter_query, twitter_no_of_tweets_to_fetch, twitter_from_date, twitter_to_date )
hh$location_lat_lon_df
runApp()
runApp()
runApp()
runApp()
setwd("E:/RUMAN/dukemedical_demo")
runApp()
event_trap_json_link = "http://vmart.machinesense.com/api/datasources/proxy/1/query?db=telegraf&q=SELECT%20%22v1%22,%22v2%22,%22v3%22,%22i1%22,%22i2%22,%22i3%22%20FROM%20%22statsd_vmart_4d%22%20WHERE%20(%22company%22%20=~%20/prop-new$/%20AND%20%22machine%22%20=~%20/EventImageTesting$/%20AND%20%22datatype%22%20=~%20/eventtrap$/)%20AND%20time%20%3E=%20now()%20-48h%20&epoch=ms"
event_trap_json = Read_JSON_From_Server( event_trap_json_link )
"series" %in% names( event_trap_json$results[[1]] )
unlist_et_jsn = unlist( event_trap_json$results[[1]]$series[[1]]$values )
TimeStamp = as.POSIXct( as.numeric( unlist_et_jsn[ seq.int( 1, length( unlist_et_jsn ), 7 ) ])/1000, origin = "1970-01-01", tz ="GMT" )
v1 = as.numeric( unlist_et_jsn[ seq.int( 2, length( unlist_et_jsn ), 7 ) ] ); v2 = as.numeric( unlist_et_jsn[ seq.int( 3, length( unlist_et_jsn ), 7 ) ] )
v3 = as.numeric( unlist_et_jsn[ seq.int( 4, length( unlist_et_jsn ), 7 ) ] ); i1 = as.numeric( unlist_et_jsn[ seq.int( 5, length( unlist_et_jsn ), 7 ) ] )
i2 = as.numeric( unlist_et_jsn[ seq.int( 6, length( unlist_et_jsn ), 7 ) ] ); i3 = as.numeric( unlist_et_jsn[ seq.int( 7, length( unlist_et_jsn ), 7 ) ] )
sine_wave_data = data.frame( TimeStamp = TimeStamp, v1 = v1, v2 = v2, v3 = v3, i1 = i1, i2 = i2, i3 = i3 )
View(sine_wave_data)
event_trap_timestamps = unique( as.character( sine_wave_data$TimeStamp ) )
event_trap_timestamps
timestamps = event_trap_timestamps[1]
split_time = unlist( strsplit( timestamps, ":" ) )
timestamps
split_time = unlist( strsplit( timestamps, ":" ) )
split_time
merge_date_hr_min = paste0( split_time[1], ":", split_time[2] )
merge_date_hr_min
event_date_hr_min = unique( unlist( lapply( event_trap_timestamps, function( timestamps ){
split_time = unlist( strsplit( timestamps, ":" ) )
merge_date_hr_min = paste0( split_time[1], ":", split_time[2] )
return( merge_date_hr_min )
} ) ) )
event_date_hr_min
event_trap_barplot_df = data.frame( TimeStamp = as.POSIXct( event_date_hr_min, origin = "1970-01-01", tz ="GMT" ), Counts = rep( 1, length( event_date_hr_min ) ) )
View(event_trap_barplot_df)
data = list()
data
event_trap_barplot_df$threshold = rep(4,nrow(event_trap_barplot_df))
View(event_trap_barplot_df)
p <- plot_ly()
p
runApp()
setwd("E:/RUMAN/new project/QARA_socialmediaanalysis")
runApp()
runApp()
runApp()
library(rsconnect)
rsconnect::setAccountInfo(name='demoappshiny',
token='DED9BFAFF82EC044D350D5F431DC464B',
secret='3LbtvJjbLdiIKjklMzKGMPJ9waEi70aA8aGNP+sg')
deployApp("E:/RUMAN/new project/QARA_socialmediaanalysis")
